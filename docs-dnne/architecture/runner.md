# DNNE Runner Interface

## Overview

The `runner.py` file is the main entry point for exported DNNE workflows. Generated by the export system, it provides a comprehensive command-line interface for controlling training, inference, checkpointing, and visualization of machine learning and robotics workflows.

## Command Line Arguments

### Training Control

#### `--timeout <duration>`
Sets the maximum training duration using flexible time formats.

**Supported formats:**
- `30s` - 30 seconds
- `5m` - 5 minutes  
- `1h` - 1 hour
- `1h30m` - 1 hour 30 minutes
- `2h45m30s` - 2 hours 45 minutes 30 seconds

**Example:**
```bash
python runner.py --timeout 5m --verbose
```

#### `--save-checkpoint-dir <directory>`
**Global enable** for checkpoint saving functionality.

**Behavior:**
- When provided: Nodes with `checkpoint_enabled=True` will save checkpoints to `{directory}/node_{id}/`
- When omitted: No checkpoints saved regardless of individual node settings
- Creates subdirectories automatically: `node_1/`, `node_6/`, etc.

**Example:**
```bash
python runner.py --save-checkpoint-dir training_checkpoints --timeout 10m
```

### Inference Control

#### `--inference`
Runs the workflow in inference mode with the following behavior:
- Disables gradient computation (`torch.no_grad()`)
- Sets all networks to evaluation mode
- Automatically loads checkpoints if `--load-checkpoint-dir` is provided
- Skips training-specific operations

#### `--load-checkpoint-dir <directory>`
**Global checkpoint loading** from the specified directory.

**Behavior:**
- Loads all available checkpoint data into respective nodes
- **Ignores per-node `checkpoint_enabled` flag during loading**
- Looks for checkpoints in `{directory}/node_{id}/model.pt` and `metadata.json`
- All checkpoint data gets loaded if it exists

**Example:**
```bash
python runner.py --inference --load-checkpoint-dir training_checkpoints --visual
```

### Visualization (Isaac Gym)

#### `--visual`
Enables Isaac Gym visual viewer, overriding the default headless mode.

**Features:**
- Creates Isaac Gym viewer window
- Real-time physics simulation visualization
- Interactive camera controls
- Frame synchronization for smooth rendering

#### `--headless`
Forces headless mode (default behavior). Useful for explicit server/cloud deployment.

### Debugging and Development

#### `--verbose` / `-v`
Enables detailed batch-level logging showing:
- Individual training step details
- Queue processing information
- Checkpoint save/load operations
- Node-level computation details

#### `--test-mode`
Runs in test mode with:
- Limited duration execution
- Performance tracking and reporting
- Additional debugging output
- Memory usage monitoring

## Checkpoint System Architecture

DNNE uses a sophisticated two-level checkpoint control system that separates global workflow control from per-node behavior.

### Two-Level Control System

#### Global Level (Command Line)
The command-line switches provide workflow-level control:

**Checkpoint Saving:**
- `--save-checkpoint-dir`: **Global enable** for all checkpoint saving
- Provides the base directory where all node checkpoints will be stored
- Without this flag, no checkpoints are saved regardless of node settings

**Checkpoint Loading:**
- `--load-checkpoint-dir`: **Global checkpoint loading** 
- Loads all available checkpoint data from the specified directory
- **Ignores per-node `checkpoint_enabled` settings during loading**
- All found checkpoint data gets loaded into respective nodes

#### Node Level (UI Configuration)
Each trainable node (Network, PPO Trainer) has individual checkpoint settings:

**Checkpoint Control:**
- `checkpoint_enabled`: Boolean flag controlling whether this specific node participates in checkpointing
- Only effective when `--save-checkpoint-dir` is provided globally
- Controls whether THIS node saves its state to checkpoints

**Checkpoint Triggers:**
- `checkpoint_trigger_type`: When to save ("epoch", "time", "best_metric")
- `checkpoint_trigger_value`: Frequency/threshold for saving

### Checkpoint Behavior Matrix

| Global Switch | Node Setting | Saving Behavior | Loading Behavior |
|---------------|--------------|-----------------|------------------|
| `--save-checkpoint-dir` provided | `checkpoint_enabled=True` | ✅ Saves checkpoints | ✅ Loads if data exists |
| `--save-checkpoint-dir` provided | `checkpoint_enabled=False` | ❌ No saving | ✅ Loads if data exists |
| No `--save-checkpoint-dir` | `checkpoint_enabled=True` | ❌ No saving | ❌ No loading |
| No `--save-checkpoint-dir` | `checkpoint_enabled=False` | ❌ No saving | ❌ No loading |

**Key Points:**
- **Saving**: Requires BOTH global switch AND per-node enable
- **Loading**: Only requires global switch, ignores per-node settings
- **Directory Structure**: Checkpoints saved to `{checkpoint_dir}/node_{id}/model.pt` + `metadata.json`

### Checkpoint Directory Structure

```
training_checkpoints/
├── node_3/                    # PPO Agent node
│   ├── model.pt              # PyTorch model state
│   └── metadata.json         # Training metadata
├── node_6/                    # PPO Trainer node  
│   ├── model.pt              # Model parameters
│   └── metadata.json         # Loss, hyperparameters, etc.
└── node_40/                   # Network node
    ├── model.pt              # Neural network weights
    └── metadata.json         # Architecture, training step
```

## Usage Examples

### Training Session
```bash
# Activate conda environment
source /home/asantanna/miniconda/bin/activate DNNE_PY38

# Run training for 5 minutes with checkpoints
cd export_system/exports/Cartpole_PPO
python runner.py --timeout 5m --save-checkpoint-dir training_checkpoints --verbose
```

### Inference with Visualization
```bash
# Run trained model with Isaac Gym viewer
cd export_system/exports/Cartpole_PPO
python runner.py --inference --visual --load-checkpoint-dir training_checkpoints --verbose
```

### Cloud/Server Training
```bash
# Headless training on server
python runner.py --timeout 30m --save-checkpoint-dir checkpoints --headless --verbose
```

### Development Testing
```bash
# Quick test run with detailed output
python runner.py --test-mode --verbose --timeout 2m
```

## Global Flags System

The runner.py coordinates with generated node code through a global flags system using Python's `builtins` module:

### Communication Mechanism
```python
# runner.py sets global flags
import builtins
builtins.VISUAL_MODE = args.visual
builtins.INFERENCE_MODE = args.inference
builtins.SAVE_CHECKPOINT_DIR = args.save_checkpoint_dir
builtins.LOAD_CHECKPOINT_DIR = args.load_checkpoint_dir
```

### Node Template Integration
Generated node code reads these flags to adapt behavior:
```python
# In generated node templates
import builtins
visual_mode = getattr(builtins, 'VISUAL_MODE', False)
inference_mode = getattr(builtins, 'INFERENCE_MODE', False)
save_dir = getattr(builtins, 'SAVE_CHECKPOINT_DIR', None)
```

### Available Global Flags
- `VISUAL_MODE`: Controls Isaac Gym viewer creation
- `HEADLESS_MODE`: Forces headless operation
- `INFERENCE_MODE`: Disables training operations
- `VERBOSE`: Controls detailed logging
- `SAVE_CHECKPOINT_DIR`: Directory for checkpoint saving
- `LOAD_CHECKPOINT_DIR`: Directory for checkpoint loading

## Isaac Gym Integration

### Visual Mode
When `--visual` is specified:
- Isaac Gym environment nodes detect the flag via `builtins.VISUAL_MODE`
- Viewer window is created with interactive camera controls
- Real-time physics simulation rendering
- Frame synchronization for smooth visualization

### Headless Mode (Default)
- No viewer creation (graphics_device = -1)
- Optimized for server/cloud deployment
- Minimal memory footprint
- Maximum training performance

### Import Order Handling
The runner.py ensures correct Isaac Gym initialization:
```python
# CRITICAL: Import isaacgym before ANY other imports
import isaacgym

# Import Isaac Gym nodes directly to prevent torch import order issues
from nodes.isaacgymenvnode_7 import IsaacGymEnvNode_7
```

## Workflow Execution

### Training Workflow
1. **Initialization**: Create all nodes with checkpoint managers
2. **Connection Setup**: Wire node inputs/outputs according to visual graph
3. **Training Loop**: Execute async queue-based training
4. **Checkpoint Saving**: Periodic saves based on node trigger settings
5. **Completion**: Final checkpoint save on timeout or completion

### Inference Workflow  
1. **Mode Setting**: Set all networks to evaluation mode
2. **Checkpoint Loading**: Load trained parameters from checkpoint directory
3. **Inference Loop**: Execute trained model without gradients
4. **Visualization**: Optional Isaac Gym viewer for real-time observation

## Performance Considerations

### Training Performance
- Use `--headless` for maximum training speed
- Adjust queue sizes based on available memory
- Consider `--timeout` to prevent infinite training
- Enable `--verbose` only for debugging (adds overhead)

### Inference Performance
- Inference mode automatically optimizes for performance
- Visual mode adds rendering overhead
- Checkpoint loading is one-time startup cost

## Troubleshooting

### Common Issues

**"PyTorch was imported before isaacgym"**
- Ensure proper conda environment activation
- Runner.py handles import order automatically

**"No checkpoint found"**
- Verify checkpoint directory path
- Check that training completed and saved checkpoints
- Ensure node IDs match between training and inference

**"Isaac Gym viewer not showing"**
- Verify X11 forwarding if using SSH
- Check that `--visual` flag is specified
- Ensure graphics drivers are properly installed

### Debug Commands
```bash
# Check checkpoint contents
ls -la training_checkpoints/*/

# Verify conda environment
python -c "import torch, isaacgym; print('Environment OK')"

# Test without visualization first
python runner.py --inference --load-checkpoint-dir training_checkpoints --headless --verbose
```

## Legacy Parameters

**Note**: The `checkpoint_load_on_start` parameter exists in some node UI configurations but is deprecated. The current architecture uses command-line switches (`--load-checkpoint-dir`) for explicit checkpoint loading control, making the per-node setting redundant.