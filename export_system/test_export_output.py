#!/usr/bin/env python3
"""
Generated by DNNE Export System
Metadata: None
"""

# Imports
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import torch
import torch.nn as nn
import torch.nn.functional as F

# Context definition
# Context class for maintaining state between nodes
from dataclasses import dataclass, field
from typing import Dict, Any

@dataclass
class Context:
    """Shared context for stateful operations"""
    memory: Dict[str, Any] = field(default_factory=dict)
    training: bool = True
    step_count: int = 0
    episode_count: int = 0
    
    def reset(self):
        """Reset context for new episode"""
        self.memory.clear()
        self.step_count = 0


# Initialize context
context = Context()

# Node implementations

# Node 1: MNISTDataset
# Template variables - replaced during export
template_vars = {'NODE_ID': 'node_1', 'DATA_PATH': './data', 'TRAIN': True, 'DOWNLOAD': True}

# Extract variables for cleaner code
NODE_ID = template_vars["NODE_ID"]
DATA_PATH = template_vars["DATA_PATH"]
TRAIN = template_vars["TRAIN"]
DOWNLOAD = template_vars["DOWNLOAD"]

# MNIST Dataset
from torchvision import datasets, transforms

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

dataset = datasets.MNIST(
    root=DATA_PATH,
    train=TRAIN,
    download=DOWNLOAD,
    transform=transform
)

# Make dataset available with node's name
globals()[NODE_ID] = dataset
globals()[f"{NODE_ID}_output"] = dataset

print(f"Loaded MNIST dataset '{NODE_ID}': {len(dataset)} samples")


# Node 2: BatchSampler
# Template variables - replaced during export
template_vars = {'NODE_ID': 'node_2', 'DATASET_VAR': 'node_1_output', 'BATCH_SIZE': 32, 'SHUFFLE': True, 'NUM_WORKERS': 0}

# Extract variables
NODE_ID = template_vars["NODE_ID"]
DATASET_VAR = template_vars["DATASET_VAR"]
BATCH_SIZE = template_vars["BATCH_SIZE"]
SHUFFLE = template_vars["SHUFFLE"]
NUM_WORKERS = template_vars["NUM_WORKERS"]

# Batch Sampler (DataLoader)
from torch.utils.data import DataLoader

# Get the dataset from the variable name
dataset = globals()[DATASET_VAR]

dataloader = DataLoader(
    dataset,
    batch_size=BATCH_SIZE,
    shuffle=SHUFFLE,
    num_workers=NUM_WORKERS
)

# Make dataloader available with node's name
globals()[NODE_ID] = dataloader
globals()[f"{NODE_ID}_output"] = dataloader

print(f"Created DataLoader '{NODE_ID}': batch_size={BATCH_SIZE}, shuffle={SHUFFLE}")



# Execution
if __name__ == "__main__":
    print("DNNE exported script")
    print(f"Loaded {len(context.memory)} components")