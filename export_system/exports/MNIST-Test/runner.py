#!/usr/bin/env python3
"""
Generated by DNNE Queue-Based Export System
Metadata: {
  "export_time": "2025-06-26 16:18:59",
  "workflow_id": "0",
  "workflow_name": "MNIST Test"
}
"""

# Imports

# Configure logging
from abc import ABC, abstractmethod
from asyncio import Queue
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from typing import Dict, Any, List, Optional
import asyncio
import logging
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(message)s')

# ==============================================================================
# Queue Framework
# ==============================================================================
# Queue-Based Node Framework
class QueueNode(ABC):
    """Base class for all queue-based nodes"""
    
    def __init__(self, node_id: str):
        self.node_id = node_id
        self.input_queues: Dict[str, Queue] = {}
        self.output_subscribers: Dict[str, List[Queue]] = {}
        self.required_inputs: List[str] = []
        self.output_names: List[str] = []
        self.running = False
        self.compute_count = 0
        self.last_compute_time = 0.0
        self.logger = logging.getLogger(f"Node.{node_id}")
    
    def setup_inputs(self, required: List[str], queue_size: int = 100):
        """Setup input queues"""
        self.required_inputs = required
        for input_name in required:
            self.input_queues[input_name] = Queue(maxsize=queue_size)
    
    def setup_outputs(self, outputs: List[str]):
        """Setup output specifications"""
        self.output_names = outputs
        for output_name in outputs:
            self.output_subscribers[output_name] = []
    
    async def send_output(self, output_name: str, value: Any):
        """Send output to all subscribers"""
        if output_name in self.output_subscribers:
            for queue in self.output_subscribers[output_name]:
                await queue.put(value)
    
    @abstractmethod
    async def compute(self, **inputs) -> Dict[str, Any]:
        """Override this to implement node logic"""
        pass
    
    async def run(self):
        """Main execution loop"""
        self.running = True
        self.logger.info(f"Starting node {self.node_id}")
        
        try:
            while self.running:
                # Gather all required inputs
                inputs = {}
                for input_name in self.required_inputs:
                    value = await self.input_queues[input_name].get()
                    inputs[input_name] = value
                
                # Execute compute
                start_time = time.time()
                outputs = await self.compute(**inputs)
                self.last_compute_time = time.time() - start_time
                self.compute_count += 1
                
                # Send outputs
                for output_name, value in outputs.items():
                    await self.send_output(output_name, value)
                    
        except asyncio.CancelledError:
            self.logger.info(f"Node {self.node_id} cancelled")
            raise
        finally:
            self.running = False


class SensorNode(QueueNode):
    """Base class for sensor nodes that generate data at fixed rates"""
    
    def __init__(self, node_id: str, update_rate: float):
        super().__init__(node_id)
        self.update_rate = update_rate
        self.update_interval = 1.0 / update_rate
    
    async def run(self):
        """Sensor run loop with fixed rate"""
        self.running = True
        self.logger.info(f"Starting sensor {self.node_id} at {self.update_rate}Hz")
        
        try:
            while self.running:
                start_time = time.time()
                
                # Execute compute
                outputs = await self.compute()
                self.compute_count += 1
                
                # Send outputs
                for output_name, value in outputs.items():
                    await self.send_output(output_name, value)
                
                # Sleep to maintain rate
                elapsed = time.time() - start_time
                sleep_time = max(0, self.update_interval - elapsed)
                if sleep_time > 0:
                    await asyncio.sleep(sleep_time)
                    
                self.last_compute_time = time.time() - start_time
                
        except asyncio.CancelledError:
            self.logger.info(f"Sensor {self.node_id} cancelled")
            raise
        finally:
            self.running = False


class GraphRunner:
    """Manages and runs the complete node graph"""
    
    def __init__(self):
        self.nodes: Dict[str, QueueNode] = {}
        self.tasks: List[asyncio.Task] = []
        self.logger = logging.getLogger("GraphRunner")
    
    def add_node(self, node: QueueNode):
        """Add a node to the graph"""
        self.nodes[node.node_id] = node
        self.logger.info(f"Added node: {node.node_id}")
    
    def wire_nodes(self, connections: List[tuple]):
        """Wire nodes together: (from_id, output, to_id, input)"""
        for from_id, output_name, to_id, input_name in connections:
            from_node = self.nodes[from_id]
            to_node = self.nodes[to_id]
            
            # Subscribe to_node's input queue to from_node's output
            from_node.output_subscribers[output_name].append(
                to_node.input_queues[input_name]
            )
            self.logger.info(f"Connected {from_id}.{output_name} -> {to_id}.{input_name}")
    
    async def run(self, duration: Optional[float] = None):
        """Run all nodes"""
        self.logger.info("Starting graph execution")
        
        # Start all nodes
        for node in self.nodes.values():
            task = asyncio.create_task(node.run())
            self.tasks.append(task)
        
        try:
            if duration:
                await asyncio.sleep(duration)
                self.logger.info(f"Stopping after {duration}s")
            else:
                # Run until cancelled
                await asyncio.gather(*self.tasks)
        except KeyboardInterrupt:
            self.logger.info("Interrupted by user")
        finally:
            # Cancel all tasks
            for task in self.tasks:
                task.cancel()
            
            # Wait for cancellation
            await asyncio.gather(*self.tasks, return_exceptions=True)
            self.logger.info("All nodes stopped")
    
    def get_stats(self) -> Dict[str, Dict[str, Any]]:
        """Get execution statistics"""
        return {
            node_id: {
                "compute_count": node.compute_count,
                "last_compute_time": node.last_compute_time,
                "running": node.running
            }
            for node_id, node in self.nodes.items()
        }


# ==============================================================================
# Node Implementations
# ==============================================================================
# Template variables - replaced during export

class MNISTDatasetNode_37(QueueNode):
    """MNIST dataset loader"""
    
    def __init__(self, node_id: str):
        super().__init__(node_id)
        self.setup_inputs(required=[])  # No inputs
        self.setup_outputs(["dataset", "schema"])
        
        # Setup dataset
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])
        
        self.dataset = datasets.MNIST(
            root="./data",
            train=True,
            download=True,
            transform=transform
        )
        
        # Create schema describing the dataset
        self.schema = {
            "outputs": {
                "images": {
                    "type": "tensor",
                    "shape": (28, 28),
                    "flattened_size": 784,
                    "dtype": "float32"
                },
                "labels": {
                    "type": "tensor", 
                    "shape": (),
                    "num_classes": 10,
                    "dtype": "int64"
                }
            },
            "num_samples": len(self.dataset)
        }
        
    async def compute(self) -> Dict[str, Any]:
        # Return dataset and its schema
        return {
            "dataset": self.dataset,
            "schema": self.schema
        }
    
    async def run(self):
        """Override run to emit dataset once"""
        self.running = True
        self.logger.info(f"Starting node {self.node_id}")
        
        try:
            # Emit dataset once
            outputs = await self.compute()
            for output_name, value in outputs.items():
                await self.send_output(output_name, value)
            
            # Keep running but don't emit again
            while self.running:
                await asyncio.sleep(1.0)
                
        except asyncio.CancelledError:
            self.logger.info(f"Node {self.node_id} cancelled")
            raise
        finally:
            self.running = False

# Template variables - replaced during export

class BatchSamplerNode_38(QueueNode):
    """Batch sampler that wraps a dataset and emits batches"""
    
    def __init__(self, node_id: str):
        super().__init__(node_id)
        self.setup_inputs(required=["dataset", "schema"])
        self.setup_outputs(["dataloader", "schema"])
        
        # Sampler parameters
        self.batch_size = 32
        self.shuffle = True
        self.seed = 325
        
    async def compute(self, dataset, schema) -> Dict[str, Any]:
        # Create dataloader from dataset
        dataloader = DataLoader(
            dataset,
            batch_size=self.batch_size,
            shuffle=self.shuffle,
            num_workers=0,
            generator=torch.Generator().manual_seed(self.seed) if self.shuffle else None
        )
        
        self.logger.info(f"Created dataloader with batch_size={self.batch_size}, shuffle={self.shuffle}")
        
        # Pass through the schema unchanged
        return {
            "dataloader": dataloader,
            "schema": schema
        }

# Template variables - replaced during export

class GetBatchNode_39(SensorNode):
    """Get batch from dataloader at fixed rate"""
    
    def __init__(self, node_id: str):
        super().__init__(node_id, update_rate=10.0)  # 10 batches per second
        self.setup_inputs(required=["dataloader", "schema"])
        self.setup_outputs(["images", "labels", "epoch_complete"])
        self.dataloader = None
        self.schema = None
        self.data_iter = None
        self.epoch = 0
        
    async def run(self):
        """Override run to wait for dataloader and schema first"""
        self.running = True
        self.logger.info(f"Starting node {self.node_id}")
        
        try:
            # Wait for dataloader and schema
            self.dataloader = await self.input_queues["dataloader"].get()
            self.schema = await self.input_queues["schema"].get()
            self.data_iter = iter(self.dataloader)
            
            # Log schema info for debugging
            if "outputs" in self.schema and "images" in self.schema["outputs"]:
                img_info = self.schema["outputs"]["images"]
                self.logger.info(f"Received dataloader with image shape: {img_info.get('shape')}, flattened_size: {img_info.get('flattened_size')}")
            
            self.logger.info("Received dataloader and schema, starting batch generation")
            
            # Now run the sensor loop
            await super().run()
            
        except asyncio.CancelledError:
            self.logger.info(f"Node {self.node_id} cancelled")
            raise
        finally:
            self.running = False
    
    async def compute(self) -> Dict[str, Any]:
        if self.data_iter is None:
            return {}
            
        epoch_complete = False
        try:
            images, labels = next(self.data_iter)
        except StopIteration:
            # Reset iterator at end of epoch
            self.epoch += 1
            epoch_complete = True
            self.data_iter = iter(self.dataloader)
            images, labels = next(self.data_iter)
            self.logger.info(f"Starting epoch {self.epoch}")
        
        return {
            "images": images,
            "labels": labels,
            "epoch_complete": epoch_complete
        }

# Template variables - replaced during export

class NetworkNode_40(QueueNode):
    """Neural network with multiple layers"""
    
    def __init__(self, node_id: str):
        super().__init__(node_id)
        self.setup_inputs(required=["input"])
        self.setup_outputs(["layers", "output", "model"])
        
        # Build network from detected layers: [{'node_id': '42', 'output_size': 128, 'bias': True, 'activation': 'relu', 'dropout': 0, 'input_size': 784}, {'node_id': '43', 'output_size': 128, 'bias': True, 'activation': 'relu', 'dropout': 0, 'input_size': 128}]
        layers = []
        layers.append(nn.Linear(784, 128, bias=True))
        layers.append(nn.ReLU())
        layers.append(nn.Linear(128, 128, bias=True))
        layers.append(nn.ReLU())
        
        self.network = nn.Sequential(*layers)
        
        # Move to GPU if available
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.network = self.network.to(self.device)
        
        self.logger.info(f"Created network with 2 layers: 784 -> 128")
        
    def get_parameters(self):
        """Return network parameters for optimizer"""
        return self.network.parameters()
        
    async def run(self):
        """Override run to emit model reference once at startup"""
        self.running = True
        self.logger.info(f"Starting node {self.node_id}")
        
        try:
            # Emit model reference once for optimizer
            await self.send_output("model", self)
            
            # Now run the normal compute loop
            while self.running:
                # Gather all required inputs
                inputs = {}
                for input_name in self.required_inputs:
                    value = await self.input_queues[input_name].get()
                    inputs[input_name] = value
                
                # Execute compute
                start_time = time.time()
                outputs = await self.compute(**inputs)
                self.last_compute_time = time.time() - start_time
                self.compute_count += 1
                
                # Send outputs (except model which was already sent)
                for output_name, value in outputs.items():
                    if output_name != "model":
                        await self.send_output(output_name, value)
                        
        except asyncio.CancelledError:
            self.logger.info(f"Node {self.node_id} cancelled")
            raise
        finally:
            self.running = False
    
    async def compute(self, input) -> Dict[str, Any]:
        # Ensure input is on correct device
        x = input.to(self.device)
        
        # Flatten if needed (for MNIST)
        if x.dim() > 2:
            x = x.view(x.size(0), -1)
        
        # Forward pass through the entire network
        output = self.network(x)
        
        return {
            "layers": None,  # This output is just for UI connectivity
            "output": output
        }


# Template variables - replaced during export

class LossNode_41(QueueNode):
    """Cross-entropy loss computation node"""
    
    def __init__(self, node_id: str):
        super().__init__(node_id)
        self.setup_inputs(required=["predictions", "labels"])
        self.setup_outputs(["loss", "accuracy"])
        
        # Setup loss function
        self.criterion = nn.CrossEntropyLoss()
    
    async def compute(self, predictions, labels) -> Dict[str, Any]:
        # Ensure tensors are on the same device
        labels = labels.to(predictions.device)
        
        # Compute loss
        loss = self.criterion(predictions, labels)
        
        # Compute accuracy for classification
        _, predicted = torch.max(predictions, 1)
        correct = (predicted == labels).sum().item()
        total = labels.size(0)
        accuracy = correct / total if total > 0 else 0.0
        
        self.logger.info(f"Loss: {loss.item():.4f}, Accuracy: {accuracy:.2%}")
        
        return {
            "loss": loss,
            "accuracy": accuracy
        }

# Template variables - replaced during export

class SGDOptimizerNode_44(QueueNode):
    """SGD Optimizer node"""
    
    def __init__(self, node_id: str):
        super().__init__(node_id)
        self.setup_inputs(required=["network"])  # Connection from network node
        self.setup_outputs(["optimizer"])
        
        # Optimizer parameters
        self.learning_rate = 0.01
        self.momentum = 0.9
        self.weight_decay = 0.0
        self.optimizer = None
        
    async def run(self):
        """Override run to wait for model connection first"""
        self.running = True
        self.logger.info(f"Starting node {self.node_id}")
        
        try:
            # Wait for network connection (network node will send itself)
            network_node = await self.input_queues["network"].get()
            
            # Create optimizer using the connected network node's parameters
            if network_node and hasattr(network_node, 'get_parameters'):
                all_params = list(network_node.get_parameters())
                
                self.optimizer = optim.SGD(
                    all_params,
                    lr=self.learning_rate,
                    momentum=self.momentum,
                    weight_decay=self.weight_decay
                )
                self.logger.info(f"Created SGD optimizer with {len(all_params)} parameter groups: lr={self.learning_rate}, momentum={self.momentum}")
                
                # Emit optimizer
                await self.send_output("optimizer", self.optimizer)
                
                # Keep running but don't emit again
                while self.running:
                    await asyncio.sleep(1.0)
            else:
                self.logger.error("No network node received - cannot create optimizer")
                
        except asyncio.CancelledError:
            self.logger.info(f"Node {self.node_id} cancelled")
            raise
        finally:
            self.running = False
    
    async def compute(self, **inputs) -> Dict[str, Any]:
        """Abstract method implementation - not used since we override run()"""
        return {}

# Template variables - replaced during export

class TrainingStepNode_45(QueueNode):
    """Training step node that performs backpropagation"""
    
    def __init__(self, node_id: str):
        super().__init__(node_id)
        self.setup_inputs(required=["loss", "optimizer"])
        self.setup_outputs(["step_complete"])
        
    async def compute(self, loss, optimizer) -> Dict[str, Any]:
        # Perform backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        self.logger.info(f"Training step completed. Loss: {loss.item():.4f}")
        
        return {"step_complete": True}

# ==============================================================================
# Main Execution
# ==============================================================================

async def main():
    """Main execution function"""
    print("ðŸš€ Starting DNNE Queue-Based Execution")
    print("=" * 60)

    # Create nodes
    node_37 = MNISTDatasetNode_37("37")
    node_38 = BatchSamplerNode_38("38")
    node_39 = GetBatchNode_39("39")
    node_40 = NetworkNode_40("40")
    node_41 = LossNode_41("41")
    node_44 = SGDOptimizerNode_44("44")
    node_45 = TrainingStepNode_45("45")

    # Create runner
    runner = GraphRunner()

    # Add nodes to runner
    runner.add_node(node_37)
    runner.add_node(node_38)
    runner.add_node(node_39)
    runner.add_node(node_40)
    runner.add_node(node_41)
    runner.add_node(node_44)
    runner.add_node(node_45)

    # Wire connections
    connections = [
        ("37", "dataset", "38", "dataset"),
        ("37", "schema", "38", "schema"),
        ("38", "dataloader", "39", "dataloader"),
        ("38", "schema", "39", "schema"),
        ("39", "images", "40", "input"),
        ("40", "output", "41", "predictions"),
        ("39", "labels", "41", "labels"),
        ("40", "model", "44", "network"),
        ("41", "loss", "45", "loss"),
        ("44", "optimizer", "45", "optimizer"),
    ]
    runner.wire_nodes(connections)

    # Run the graph
    try:
        # Run indefinitely (Ctrl+C to stop)
        await runner.run()
        # Or run for specific duration:
        # await runner.run(duration=10.0)  # Run for 10 seconds
    except KeyboardInterrupt:
        print('\nðŸ›‘ Stopped by user')

    # Show final statistics
    print('\nðŸ“Š Final Statistics:')
    stats = runner.get_stats()
    for node_id, node_stats in stats.items():
        print(f'  {node_id}: {node_stats["compute_count"]} computations, '
              f'avg time: {node_stats["last_compute_time"]:.3f}s')


if __name__ == '__main__':
    asyncio.run(main())