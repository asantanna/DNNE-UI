#!/usr/bin/env python3
"""
Generated by DNNE Export System
Metadata: None
"""

# Imports
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim

# Context definition
# Context class for maintaining state between nodes
from dataclasses import dataclass, field
from typing import Dict, Any

@dataclass
class Context:
    """Shared context for stateful operations"""
    memory: Dict[str, Any] = field(default_factory=dict)
    training: bool = True
    step_count: int = 0
    episode_count: int = 0
    
    def reset(self):
        """Reset context for new episode"""
        self.memory.clear()
        self.step_count = 0


# Initialize context
context = Context()

# Node implementations

# Node 1: MNISTDataset
# Template variables - replaced during export
template_vars = {'NODE_ID': 'node_1', 'DATA_PATH': './data', 'TRAIN': True, 'DOWNLOAD': True}

# Extract variables for cleaner code
NODE_ID = template_vars["NODE_ID"]
DATA_PATH = template_vars["DATA_PATH"]
TRAIN = template_vars["TRAIN"]
DOWNLOAD = template_vars["DOWNLOAD"]

# MNIST Dataset
from torchvision import datasets, transforms

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

dataset = datasets.MNIST(
    root=DATA_PATH,
    train=TRAIN,
    download=DOWNLOAD,
    transform=transform
)

# Make dataset available with node's name
globals()[NODE_ID] = dataset
globals()[f"{NODE_ID}_output"] = dataset

print(f"Loaded MNIST dataset '{NODE_ID}': {len(dataset)} samples")


# Node 2: BatchSampler
# Template variables - replaced during export
template_vars = {'NODE_ID': 'node_2', 'DATASET_VAR': 'node_1_output', 'BATCH_SIZE': 32, 'SHUFFLE': True, 'NUM_WORKERS': 0}

# Extract variables
NODE_ID = template_vars["NODE_ID"]
DATASET_VAR = template_vars["DATASET_VAR"]
BATCH_SIZE = template_vars["BATCH_SIZE"]
SHUFFLE = template_vars["SHUFFLE"]
NUM_WORKERS = template_vars["NUM_WORKERS"]

# Batch Sampler (DataLoader)
from torch.utils.data import DataLoader

# Get the dataset from the variable name
dataset = globals()[DATASET_VAR]

dataloader = DataLoader(
    dataset,
    batch_size=BATCH_SIZE,
    shuffle=SHUFFLE,
    num_workers=NUM_WORKERS
)

# Make dataloader available with node's name
globals()[NODE_ID] = dataloader
globals()[f"{NODE_ID}_output"] = dataloader

print(f"Created DataLoader '{NODE_ID}': batch_size={BATCH_SIZE}, shuffle={SHUFFLE}")



# Node 3: LinearLayer
# Template variables - replaced during export
template_vars = {'NODE_ID': 'node_3', 'INPUT_SIZE': -1, 'OUTPUT_SIZE': 128, 'ACTIVATION': 'relu', 'DROPOUT': 0.5, 'BIAS': True, 'WEIGHT_INIT': 'xavier'}

# Extract variables
NODE_ID = template_vars["NODE_ID"]
INPUT_SIZE = template_vars["INPUT_SIZE"]
OUTPUT_SIZE = template_vars["OUTPUT_SIZE"]
ACTIVATION = template_vars["ACTIVATION"]
DROPOUT = template_vars["DROPOUT"]
BIAS = template_vars["BIAS"]
WEIGHT_INIT = template_vars["WEIGHT_INIT"]

# Store layer configuration
globals()[f"{NODE_ID}_config"] = {
    "type": "Linear",
    "input_size": INPUT_SIZE,
    "output_size": OUTPUT_SIZE,
    "activation": ACTIVATION,
    "dropout": DROPOUT,
    "bias": BIAS,
    "weight_init": WEIGHT_INIT
}

print(f"Configured linear layer '{NODE_ID}': input_size={INPUT_SIZE} -> {OUTPUT_SIZE}, activation={ACTIVATION}")


# Node 4: LinearLayer
# Template variables - replaced during export
template_vars = {'NODE_ID': 'node_4', 'INPUT_SIZE': -1, 'OUTPUT_SIZE': 10, 'ACTIVATION': 'none', 'DROPOUT': 0.0, 'BIAS': True, 'WEIGHT_INIT': 'xavier'}

# Extract variables
NODE_ID = template_vars["NODE_ID"]
INPUT_SIZE = template_vars["INPUT_SIZE"]
OUTPUT_SIZE = template_vars["OUTPUT_SIZE"]
ACTIVATION = template_vars["ACTIVATION"]
DROPOUT = template_vars["DROPOUT"]
BIAS = template_vars["BIAS"]
WEIGHT_INIT = template_vars["WEIGHT_INIT"]

# Store layer configuration
globals()[f"{NODE_ID}_config"] = {
    "type": "Linear",
    "input_size": INPUT_SIZE,
    "output_size": OUTPUT_SIZE,
    "activation": ACTIVATION,
    "dropout": DROPOUT,
    "bias": BIAS,
    "weight_init": WEIGHT_INIT
}

print(f"Configured linear layer '{NODE_ID}': input_size={INPUT_SIZE} -> {OUTPUT_SIZE}, activation={ACTIVATION}")


# Node 5: SGDOptimizer
# Template variables - replaced during export
template_vars = {'NODE_ID': 'node_5', 'LEARNING_RATE': 0.01, 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.0}

# Extract variables
NODE_ID = template_vars["NODE_ID"]
LEARNING_RATE = template_vars["LEARNING_RATE"]
MOMENTUM = template_vars["MOMENTUM"]
WEIGHT_DECAY = template_vars["WEIGHT_DECAY"]

# SGD Optimizer configuration
globals()[f"{NODE_ID}_config"] = {
    "type": "SGD",
    "lr": LEARNING_RATE,
    "momentum": MOMENTUM,
    "weight_decay": WEIGHT_DECAY
}

print(f"Configured SGD optimizer '{NODE_ID}': lr={LEARNING_RATE}, momentum={MOMENTUM}")


# Execution
if __name__ == "__main__":
    print("DNNE exported script")
    print(f"Loaded {len(context.memory)} components")